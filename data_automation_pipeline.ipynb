{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba8bd22-90a0-42a4-8eca-07ea24bdc9c7",
   "metadata": {},
   "source": [
    "## Untidy to Tidy Data Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd212805-6a61-47da-84f2-89c418de4c1d",
   "metadata": {},
   "source": [
    "Because this notebook extracts sensistive data from the host file, most outputs have been cleared before committing. I am only sharing my code so that all sensitive information is thoroughly excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f04aad6-1a8e-4867-92bf-b347eec333c4",
   "metadata": {},
   "source": [
    "Author: Jadi Smith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d57bc89-8801-4d70-b1a5-3ed6060a1384",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e04ed9c0-06d6-4080-bff3-8b415713128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6351571-af51-463b-aabc-d214f96238e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the pages that i need\n",
    "use_pages = []\n",
    "with pdfplumber.open(\"data/071825D.pdf\") as pdf:\n",
    "   for page in pdf.pages:\n",
    "       text = page.extract_text()\n",
    "       if text and \"N198 N257\" in text:\n",
    "           use_pages.append(page.page_number)\n",
    "\n",
    "use_pages.sort()\n",
    "\n",
    "#excluding data from the pages that i do not need\n",
    "excluded_words = [\n",
    "   \"TOTAL CMS\",\n",
    "   \"HEADER EOBS\",\n",
    "   \"LNN FROM DTE-THRU DTE POS SPEC PROC CD M1 M2 M3 M4 UNITS BILLED/ALLWD BILLED ALLOWED COB PAID STATUS\",\n",
    "   \"HEADER REMARK CODES: N198 N257\",\n",
    "   \"------------------------------------------------------------------------------------------------------------------------------------------------------\",\n",
    "]\n",
    "\n",
    "#functions to extract cleaned lines from each page\n",
    "def get_cleaned_lines():\n",
    "   all_lines = []\n",
    "   with pdfplumber.open(\"data/071825D.pdf\") as pdf:\n",
    "       for page_num in use_pages:\n",
    "           text = pdf.pages[page_num - 1].extract_text()\n",
    "           if text:\n",
    "               lines = text.splitlines()\n",
    "               for idx, line in enumerate(lines):\n",
    "                   if (idx == 9 or idx >= 13) and not any(word in line.upper() for word in excluded_words):\n",
    "                       all_lines.append(line)\n",
    "   return all_lines\n",
    "\n",
    "cleaned_data = get_cleaned_lines()\n",
    "\n",
    "#icn lines\n",
    "def extract_icn(lines):\n",
    "   result = {\"icn\": [], \"member_id\": [], \"member_name\": [], \"billed_date\": [], \"p_auth_no\": [], \"patient_number\": []}\n",
    "   for line in lines:\n",
    "       parts = line.strip().split()\n",
    "       if len(parts) >= 6 and parts[0].isdigit() and len(parts[0]) == 13:\n",
    "           result[\"icn\"].append(parts[0])\n",
    "           result[\"member_id\"].append(parts[1])\n",
    "           \n",
    "           name_parts = []\n",
    "           for part in parts[2:]:\n",
    "               if part[0].isdigit():\n",
    "                   break\n",
    "               name_parts.append(part)\n",
    "           full_name = \" \".join(name_parts)\n",
    "           result[\"member_name\"].append(full_name)\n",
    "           \n",
    "           billed_date_index = 2 + len(name_parts) #bc the billed date is always after the 2nd idx name part\n",
    "           result[\"billed_date\"].append(parts[billed_date_index])\n",
    "           \n",
    "           if len(parts[-2]) == 12:\n",
    "               result[\"p_auth_no\"].append(parts[-2])\n",
    "           else:\n",
    "               result[\"p_auth_no\"].append(\"\")\n",
    "               \n",
    "           result[\"patient_number\"].append(parts[-1])\n",
    "   return result\n",
    "    \n",
    "#cos line\n",
    "def extract_cos(lines):\n",
    "   result = {\n",
    "       \"cos\": [], \"from_date\": [], \"thru_date\": [], \"billed\": [], \"allowed\": [],\n",
    "       \"copay_deduct\": [], \"pt_liab\": [], \"cob\": [], \"tot_paid\": [], \"status\": []\n",
    "   }\n",
    "   for line in lines:\n",
    "       parts = line.strip().split()\n",
    "       if len(parts) >= 10 and parts[0].isdigit() and len(parts[0]) == 3:\n",
    "           result[\"cos\"].append(parts[0])\n",
    "           result[\"from_date\"].append(parts[1])\n",
    "           result[\"thru_date\"].append(parts[2])\n",
    "           result[\"billed\"].append(parts[3])\n",
    "           result[\"allowed\"].append(parts[4])\n",
    "           result[\"copay_deduct\"].append(parts[5])\n",
    "           result[\"pt_liab\"].append(parts[6])\n",
    "           result[\"cob\"].append(parts[7])\n",
    "           result[\"tot_paid\"].append(parts[8])\n",
    "           result[\"status\"].append(parts[9])\n",
    "   return result\n",
    "    \n",
    "# building df to export to csv\n",
    "icn_df = pd.DataFrame(extract_icn(cleaned_data))\n",
    "cos_df = pd.DataFrame(extract_cos(cleaned_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca33112e-3262-47d8-a378-1a1ae924f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#npi lines\n",
    "all_npi_data = {\"npi\":[]}\n",
    "def npi_line():\n",
    "   for page in use_pages:\n",
    "       page_text = pdf.pages[page - 1].extract_text()\n",
    "       lines = page_text.splitlines()\n",
    "       #npi for the page\n",
    "       current_npi = \"\"\n",
    "       for line in lines:\n",
    "           if \"NPI\" in line:\n",
    "               parts = line.strip().split()\n",
    "               for i, p in enumerate(parts):\n",
    "                   if p == \"NPI\" and i + 1 < len(parts):\n",
    "                       current_npi = parts[i + 1]\n",
    "       # matching npi per icn line\n",
    "       for line in lines:\n",
    "           parts = line.strip().split()\n",
    "           if len(parts) > 0 and len(parts[0]) == 13: \n",
    "               all_npi_data[\"npi\"].append(current_npi)\n",
    "   return all_npi_data\n",
    "results = npi_line()\n",
    "\n",
    "npi_df = pd.DataFrame(all_npi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "104e72ef-6a82-4ac7-90de-6efafaab515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lnn lines\n",
    "def extract_lnn(lines):\n",
    "   result = {\n",
    "       \"lnn\": [], \"from_date\": [], \"thru_date\": [],\n",
    "       \"pos\": [], \"spec\": [], \"proc_cd\": [], \"m1\": [],\n",
    "       \"m2\": [], \"m3\": [], \"m4\": [],\n",
    "       \"units_billed\": [], \"units_allowed\": [],\n",
    "       \"billed\": [], \"allowed\": [], \"cob\": [],\n",
    "       \"paid\": [], \"status\": []\n",
    "   }\n",
    "   found_lnn = False\n",
    "   for line in lines:\n",
    "       if line.startswith(\"1 \"):\n",
    "           found_lnn = True\n",
    "           parts = line.split()\n",
    "           # starting columns\n",
    "           result[\"lnn\"].append(parts[0])\n",
    "           result[\"from_date\"].append(parts[1])\n",
    "           result[\"thru_date\"].append(parts[2])\n",
    "           result[\"pos\"].append(parts[3])\n",
    "           result[\"spec\"].append(parts[4])\n",
    "           result[\"proc_cd\"].append(parts[5])\n",
    "           result[\"m1\"].append(parts[6])\n",
    "           # m1-4 exist\n",
    "           if all(len(p) == 2 for p in parts[7:10]):\n",
    "               result[\"m2\"].append(parts[7])\n",
    "               result[\"m3\"].append(parts[8])\n",
    "               result[\"m4\"].append(parts[9])\n",
    "               result[\"units_billed\"].append(parts[10])\n",
    "               result[\"units_allowed\"].append(parts[11])\n",
    "               result[\"billed\"].append(parts[12])\n",
    "               result[\"allowed\"].append(parts[13])\n",
    "           # m1-3 exist\n",
    "           elif all(len(p) == 2 for p in parts[7:9]) and len(parts[9]) != 2:\n",
    "               result[\"m2\"].append(parts[7])\n",
    "               result[\"m3\"].append(parts[8])\n",
    "               result[\"m4\"].append(\"\")\n",
    "               result[\"units_billed\"].append(parts[9])\n",
    "               result[\"units_allowed\"].append(parts[10])\n",
    "               result[\"billed\"].append(parts[11])\n",
    "               result[\"allowed\"].append(parts[12])\n",
    "           # m1-2 exist\n",
    "           elif len(parts[7]) == 2 and len(parts[8]) != 2:\n",
    "               result[\"m2\"].append(parts[7])\n",
    "               result[\"m3\"].append(\"\")\n",
    "               result[\"m4\"].append(\"\")\n",
    "               result[\"units_billed\"].append(parts[8])\n",
    "               result[\"units_allowed\"].append(parts[9])\n",
    "               result[\"billed\"].append(parts[10])\n",
    "               result[\"allowed\"].append(parts[11])\n",
    "           # m1 only exists\n",
    "           else:\n",
    "               result[\"m2\"].append(\"\")\n",
    "               result[\"m3\"].append(\"\")\n",
    "               result[\"m4\"].append(\"\")\n",
    "               result[\"units_billed\"].append(parts[7])\n",
    "               result[\"units_allowed\"].append(parts[8])\n",
    "               result[\"billed\"].append(parts[9])\n",
    "               result[\"allowed\"].append(parts[10])\n",
    "           # always last three columns\n",
    "           result[\"cob\"].append(parts[-3])\n",
    "           result[\"paid\"].append(parts[-2])\n",
    "           result[\"status\"].append(parts[-1])\n",
    "   # if no lnn line at all, add one empty row\n",
    "   if not found_lnn:\n",
    "       for key in result:\n",
    "           result[key].append(\"\")\n",
    "   return result\n",
    "lnn_df = pd.DataFrame(extract_lnn(cleaned_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa475827-306b-43ab-a73c-d482f971ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lnn_df.to_csv(\"lnn.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbb491d0-3c5f-4904-8125-18b1e993be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([npi_df,lnn_df,icn_df, cos_df], axis=1)\n",
    "final_df.to_csv(\"cleaned_billing_pdf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7fb785f-379c-4c33-b905-175d581d2e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([icn_df, cos_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db557092-6e7e-4e29-b72e-f870d7cd6aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
